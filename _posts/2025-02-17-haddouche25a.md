---
title: A PAC-Bayesian Link Between Generalisation and Flat Minima
abstract: Modern machine learning usually involves predictors in the overparameterised
  setting (number of trained parameters greater than dataset size), and their training
  yields not only good performance on training data, but also good generalisation
  capacity. This phenomenon challenges many theoretical results, and remains an open
  problem. To reach a better understanding, we provide novel generalisation bounds
  involving gradient terms. To do so, we combine the PAC-Bayes toolbox with Poincar√©
  and Log-Sobolev inequalities, avoiding an explicit dependency on the dimension of
  the predictor space. Our results highlight the positive influence of flat minima
  (being minima with a neighbourhood nearly minimising the learning problem as well)
  on generalisation performance, involving directly the benefits of the optimisation
  phase.
openreview: P4OJds0sdD
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: haddouche25a
month: 0
tex_title: A PAC-Bayesian Link Between Generalisation and Flat Minima
firstpage: 481
lastpage: 511
page: 481-511
order: 481
cycles: false
bibtex_author: Haddouche, Maxime and Viallard, Paul and Simsekli, Umut and Guedj,
  Benjamin
author:
- given: Maxime
  family: Haddouche
- given: Paul
  family: Viallard
- given: Umut
  family: Simsekli
- given: Benjamin
  family: Guedj
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/haddouche25a/haddouche25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
