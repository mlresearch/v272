---
title: Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits
  Problem
abstract: 'We give nearly-tight upper and lower bounds for the <em>improving multi-armed
  bandits</em> problem. An instance of this problem has $k$ arms, each of whose reward
  function is a concave and increasing function of the <em>number of times that arm
  has been pulled so far</em>. We show that for any randomized online algorithm, there
  exists an instance on which it must suffer at least an $\Omega(\sqrt{k})$ approximation
  factor relative to the optimal reward. We then provide a randomized online algorithm
  that guarantees an $O(\sqrt{k})$ approximation factor, if it is told the maximum
  reward achievable by the optimal arm in advance. We then show how to remove this
  assumption at the cost of an extra $O(\log k)$ approximation factor, achieving an
  overall $O(\sqrt{k} \log k)$ approximation relative to optimal. '
openreview: oytwNAlEfY
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: blum25a
month: 0
tex_title: Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits
  Problem
firstpage: 228
lastpage: 245
page: 228-245
order: 228
cycles: false
bibtex_author: Blum, Avrim and Ravichandran, Kavya
author:
- given: Avrim
  family: Blum
- given: Kavya
  family: Ravichandran
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/blum25a/blum25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
