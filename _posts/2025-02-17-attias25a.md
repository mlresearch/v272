---
title: Sample Compression Scheme Reductions
abstract: 'We present novel reductions from sample compression schemes in multiclass
  classification, regression, and adversarially robust learning settings to binary
  sample compression schemes. Assuming we have a compression scheme for binary classes
  of size $f(d_\mathrm{VC})$, where $d_\mathrm{VC}$ is the VC dimension, then we have
  the following results: (1) If the binary compression scheme is a majority vote or
  a stable compression scheme, then there exists a multiclass compression scheme of
  size $O(f(d_\mathrm{G}))$, where $d_\mathrm{G}$ is the graph dimension. Moreover,
  for general binary compression schemes, we obtain a compression of size $O(f(d_\mathrm{G})\log|\mathcal{Y}|)$,
  where $\mathcal{Y}$ is the label space. (2) If the binary compression scheme is
  a majority vote or a stable compression scheme, then there exists an $\epsilon$-approximate
  compression scheme for regression over $[0,1]$-valued functions of size $O(f(d_\mathrm{P}))$,
  where $d_\mathrm{P}$ is the pseudo-dimension. For general binary compression schemes,
  we obtain a compression of size $O(f(d_\mathrm{P})\log(1/\epsilon))$. These results
  would have significant implications if the sample compression conjecture, which
  posits that any binary concept class with a finite VC dimension admits a binary
  compression scheme of size $O(d_\mathrm{VC})$, is resolved (Littlestone and Warmuth,
  1986; Floyd and Warmuth, 1995; Warmuth, 2003). Our results would then extend the
  proof of the conjecture immediately to other settings. We establish similar results
  for adversarially robust learning and also provide an example of a concept class
  that is robustly learnable but has no bounded-size compression scheme, demonstrating
  that learnability is not equivalent to having a compression scheme independent of
  the sample size, unlike in binary classification, where compression of size $2^{O(d_\mathrm{VC})}$
  is attainable (Moran and Yehudayoff, 2016).'
openreview: uQsn9AZPwR
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: attias25a
month: 0
tex_title: Sample Compression Scheme Reductions
firstpage: 134
lastpage: 162
page: 134-162
order: 134
cycles: false
bibtex_author: Attias, Idan and Hanneke, Steve and Ramaswami, Arvind
author:
- given: Idan
  family: Attias
- given: Steve
  family: Hanneke
- given: Arvind
  family: Ramaswami
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/attias25a/attias25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
