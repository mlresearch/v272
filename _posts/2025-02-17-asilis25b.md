---
title: Proper Learnability and the Role of Unlabeled Data
abstract: 'Proper learning refers to the setting in which learners must emit predictors
  in the underlying hypothesis class $\mathcal{H}$, and often leads to learners with
  simple algorithmic forms (e.g., empirical risk minimization (ERM), structural risk
  minimization (SRM)). The limitation of proper learning, however, is that there exist
  problems which can only be learned improperly, e.g. in multiclass classification.
  Thus, we ask: Under what assumptions on the hypothesis class or the information
  provided to the learner is a problem properly learnable? We first demonstrate that
  when the unlabeled data distribution is given, there always exists an optimal proper
  learner governed by \emph{distributional regularization}, a randomized generalization
  of regularization. We refer to this setting as the \emph{distribution-fixed} PAC
  model, and continue to evaluate the learner on its worst-case performance over all
  distributions. Our result holds for all metric loss functions and any finite learning
  problem (with no dependence on its size). Further, we demonstrate that sample complexities
  in the distribution-fixed PAC model can shrink by only a logarithmic factor from
  the classic PAC model, strongly refuting the role of unlabeled data in PAC learning
  (from a worst-case perspective). We complement this with impossibility results which
  obstruct any characterization of proper learnability in the classic (realizable)
  PAC model. First, we observe that there are problems whose proper learnability is
  logically \emph{undecidable}, i.e., independent of the ZFC axioms. We then show
  that proper learnability is not a monotone property of the underlying hypothesis
  class, and that it is not a \emph{local} property (in a precise sense). We also
  point out how the non-monotonicity of proper learning obstructs relaxations of the
  distribution-fixed model that preserve proper learnability, including natural notions
  of class-conditional learning of the unlabeled data distribution.  Our impossibility
  results all hold even for the fundamental setting of multiclass classification,
  and go through a reduction of EMX learning (Ben-David et al., 2019) to proper classification
  which may be of independent interest.'
openreview: lEhg21XO0R
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: asilis25b
month: 0
tex_title: Proper Learnability and the Role of Unlabeled Data
firstpage: 112
lastpage: 133
page: 112-133
order: 112
cycles: false
bibtex_author: Asilis, Julian and Devic, Siddartha and Dughmi, Shaddin and Sharan,
  Vatsal and Teng, Shang-Hua
author:
- given: Julian
  family: Asilis
- given: Siddartha
  family: Devic
- given: Shaddin
  family: Dughmi
- given: Vatsal
  family: Sharan
- given: Shang-Hua
  family: Teng
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/asilis25b/asilis25b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
