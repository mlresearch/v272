---
title: How rotation invariant algorithms are fooled by noise on sparse targets
abstract: It is well known that rotation invariant algorithms are sub-optimal for
  learning sparse linear problems, when the number of examples is below the input
  dimension. This includes any gradient descent trained neural net with a fully-connected
  input layer initialized with a rotationally symmetric distribution. The simplest
  sparse problem is learning a single feature out of d features. In that case the
  classification error or regression loss of rotation invariant algorithms grows with
  1âˆ’n/d, where n is the number of examples seen. These lower bounds become vacuous
  when the number of examples n reaches the dimension d. After d examples, the gradient
  space has full rank and any weight vector can be expressed, including the unit vector
  that determines the target feature. In this work, we show that when noise is added
  to this sparse linear problem, rotation invariant algorithms are still sub-optimal
  after seeing d or more examples. We prove this via a lower bound for the Bayes optimal
  algorithm on a rotationally symmetrized problem. We then prove much better upper
  bounds on the same problem for a large variety of algorithms that are non-invariant
  by rotations. Finally, we analyze the gradient flow trajectories of many standard
  optimization algorithms (such as AdaGrad) on the same noisy feature learning problem,
  and show how they veer away from the noisy sparse targets. We then contrast them
  with a group of non-rotation invariant algorithms that veer towards the sparse targets.
  We believe that the lower bounds method and trajectory categorization will be crucial
  for analyzing other families of algorithms with different classes of invariances.
openreview: GSgcnyQzpz
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: warmuth25a
month: 0
tex_title: How rotation invariant algorithms are fooled by noise on sparse targets
firstpage: 1316
lastpage: 1360
page: 1316-1360
order: 1316
cycles: false
bibtex_author: Warmuth, Manfred K. and Kot{\polishl}owski, Wojciech and Jones, Matt
  and Amid, Ehsan
author:
- given: Manfred K.
  family: Warmuth
- given: Wojciech
  family: Kot\polishlowski
- given: Matt
  family: Jones
- given: Ehsan
  family: Amid
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/warmuth25a/warmuth25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
