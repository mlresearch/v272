---
title: Enhanced $H$-Consistency Bounds
abstract: Recent research has introduced a key notion of $H$-consistency bounds for
  surrogate losses. These bounds offer finite-sample guarantees, quantifying the relationship
  between the zero-one estimation error (or other target loss) and the surrogate loss
  estimation error for a specific hypothesis set. However, previous bounds were derived
  under the condition that a lower bound of the surrogate loss conditional regret
  is given as a convex function of the target conditional regret, without non-constant
  factors depending on the predictor or input instance. Can we derive finer and more
  favorable $H$-consistency bounds? In this work, we relax this condition and present
  a general framework for establishing <em>enhanced $H$-consistency bounds</em> based
  on more general inequalities relating conditional regrets. Our theorems not only
  subsume existing results as special cases but also enable the derivation of more
  favorable bounds in various scenarios. These include standard multi-class classification,
  binary and multi-class classification under Tsybakov noise conditions, and bipartite
  ranking.
openreview: qgnVGFJMJo
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mao25a
month: 0
tex_title: Enhanced $H$-Consistency Bounds
firstpage: 772
lastpage: 813
page: 772-813
order: 772
cycles: false
bibtex_author: Mao, Anqi and Mohri, Mehryar and Zhong, Yutao
author:
- given: Anqi
  family: Mao
- given: Mehryar
  family: Mohri
- given: Yutao
  family: Zhong
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/mao25a/mao25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
