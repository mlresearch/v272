---
title: 'The Dimension Strikes Back with Gradients: Generalization of Gradient Methods
  in Stochastic Convex Optimization'
abstract: We study the generalization performance of gradient methods in the fundamental
  stochastic convex optimization setting, focusing on its dimension dependence.  First,
  for full-batch gradient descent (GD) we give a construction of a learning problem
  in dimension $d=O(n^2)$, where the canonical version of GD (tuned for optimal performance
  on the empirical risk) trained with $n$ training examples converges, with constant
  probability, to an approximate empirical risk minimizer with $\Omega(1)$ population
  excess risk. Our bound translates to a lower bound of $\smash{\Omega(\sqrt{d})}$
  on the number of training examples required for standard GD to reach a non-trivial
  test error, answering an open question raised by Feldman (2016) and Amir, Koren
  and Livni (2021) and showing that a non-trivial dimension dependence is unavoidable.
  Furthermore, for standard one-pass stochastic gradient descent (SGD), we show that
  an application of the same construction technique provides a similar $\smash{\Omega(\sqrt{d})}$
  lower bound for the sample complexity of SGD to reach a non-trivial empirical error,
  despite achieving optimal test performance. This again provides for an exponential
  improvement in the dimension dependence compared to previous work (Koren et al.,
  2022), resolving an open question left therein.
openreview: Y08eqbmCDD
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: schliserman25a
month: 0
tex_title: 'The Dimension Strikes Back with Gradients: Generalization of Gradient
  Methods in Stochastic Convex Optimization'
firstpage: 1041
lastpage: 1107
page: 1041-1107
order: 1041
cycles: false
bibtex_author: Schliserman, Matan and Sherman, Uri and Koren, Tomer
author:
- given: Matan
  family: Schliserman
- given: Uri
  family: Sherman
- given: Tomer
  family: Koren
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/schliserman25a/schliserman25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
