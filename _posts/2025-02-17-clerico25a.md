---
title: Generalisation under gradient descent via deterministic PAC-Bayes
abstract: We establish disintegrated PAC-Bayesian generalisation bounds for models
  trained with gradient descent methods or continuous gradient flows. Contrary to
  standard practice in the PAC-Bayesian setting, our result applies to optimisation
  algorithms that are deterministic, without requiring any <em>de-randomisation</em>
  step. Our bounds are fully computable, depending on the density of the initial distribution
  and the Hessian of the training objective over the trajectory. We show that our
  framework can be applied to a variety of iterative optimisation algorithms, including
  stochastic gradient descent (SGD), momentum-based schemes, and damped Hamiltonian
  dynamics.
openreview: L57EeV3VKf
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: clerico25a
month: 0
tex_title: Generalisation under gradient descent via deterministic PAC-Bayes
firstpage: 349
lastpage: 389
page: 349-389
order: 349
cycles: false
bibtex_author: Clerico, Eugenio and Farghly, Tyler and Deligiannidis, George and Guedj,
  Benjamin and Doucet, Arnaud
author:
- given: Eugenio
  family: Clerico
- given: Tyler
  family: Farghly
- given: George
  family: Deligiannidis
- given: Benjamin
  family: Guedj
- given: Arnaud
  family: Doucet
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/clerico25a/clerico25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
