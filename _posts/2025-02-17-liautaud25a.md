---
title: Minimax-optimal and Locally-adaptive Online Nonparametric Regression
abstract: We study adversarial online nonparametric regression with general convex
  losses and propose a parameter-free learning algorithm that achieves minimax optimal
  rates. Our approach leverages chaining trees to compete against Hölder functions
  and establishes optimal regret bounds. While competing with nonparametric function
  classes can be challenging, they often exhibit local patterns - such as local Hölder
  continuity - that online algorithms can exploit. Without prior knowledge, our method
  dynamically tracks and adapts to different Hölder profiles by pruning a core chaining
  tree structure, aligning itself with local smoothness variations. This leads to
  the first computationally efficient algorithm with locally adaptive optimal rates
  for online regression in an adversarial setting. Finally, we discuss how these notions
  could be extended to a boosting framework, offering promising directions for future
  research.
openreview: tKx9a84i8A
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liautaud25a
month: 0
tex_title: Minimax-optimal and Locally-adaptive Online Nonparametric Regression
firstpage: 702
lastpage: 735
page: 702-735
order: 702
cycles: false
bibtex_author: Liautaud, Paul and Gaillard, Pierre and Wintenberger, Olivier
author:
- given: Paul
  family: Liautaud
- given: Pierre
  family: Gaillard
- given: Olivier
  family: Wintenberger
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/liautaud25a/liautaud25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
