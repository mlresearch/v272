---
title: On Generalization Bounds for Neural Networks with Low Rank Layers
abstract: While previous optimization results have suggested that deep neural networks
  tend to favour low-rank weight matrices, the implications of this inductive bias
  on generalization bounds remain underexplored. In this paper, we apply a chain rule
  for Gaussian complexity (Maurer, 2016a) to analyze how low-rank layers in deep networks
  can prevent the accumulation of rank and dimensionality factors that typically multiply
  across layers. This approach yields generalization bounds for rank and spectral
  norm constrained networks. We compare our results to prior generalization bounds
  for deep networks, highlighting how deep networks with low-rank layers can achieve
  better generalization than those with full-rank layers. Additionally, we discuss
  how this framework provides new perspectives on the generalization capabilities
  of deep networks exhibiting neural collapse.
openreview: TAvypH5yl5
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pinto25a
month: 0
tex_title: On Generalization Bounds for Neural Networks with Low Rank Layers
firstpage: 921
lastpage: 936
page: 921-936
order: 921
cycles: false
bibtex_author: Pinto, Andrea and Rangamani, Akshay and Poggio, Tomaso A
author:
- given: Andrea
  family: Pinto
- given: Akshay
  family: Rangamani
- given: Tomaso A
  family: Poggio
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/pinto25a/pinto25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
