---
title: On the Hardness of Learning One Hidden Layer Neural Networks
abstract: 'In this work, we consider the problem of learning one hidden layer ReLU
  neural networks with inputs from $\mathbb{R}^d$. It is well known due to (Klivans
  and Sherstov, 2009) that without further assumptions on the distribution $\mathcal{D}$,
  e.g., when $\mathcal{D}$ can be supported over the Boolean hypercube, learning even
  one-hidden layer neural networks is impossible (or “hard”) for polynomial-time estimators
  under standard cryptographic assumptions. Given the success of neural networks in
  practice, a long line of recent work has attempted to study instead the canonical
  continuous input distribution case where $\mathcal{D}$ is the isotropic Gaussian,
  i.e., $\mathcal{D}=\mathcal{N}\left(0,I_d\right)$ which is also the setting that
  we follow in this work. Yet, despite a long line of research, it remains open whether
  there is a polynomial-time algorithm for learning one hidden layer neural networks
  when $\mathcal{D} = \mathcal{N}\left(0,I_d\right)$. It is known that a single neuron,
  i.e., zero hidden layer neural network, can be learned in polynomial time (Zarifis
  et al., 2024), while neural networks with more than two hidden layers are hard to
  learn (Chen et al., 2022). Nevertheless, the case of one hidden layer neural networks
  is not well understood.  In this paper we close this gap in the literature by answering
  the question of efficient learnability of neural networks with one hidden layer.
  We establish that under the CLWE assumption from cryptography (Bruna et al., 2021),
  learning the class of one hidden layer neural network with polynomial size under
  standard Gaussian inputs and polynomially small Gaussian noise is indeed computationally
  hard. Importantly, solving CLWE in polynomial time implies a polynomial-time quantum
  algorithm that solves the <em>worst-case</em> gap shortest vector problem (GapSVP)
  within polynomial factors, a widely believed hard task in cryptography and algorithmic
  theory of lattices (Micciancio and Regev, 2009). En route, we prove the hardness
  of learning Lipschitz periodic functions under standard Gaussian inputs and polynomially
  small Gaussian noise. This improves the previous result from (Song et al., 2021),
  which proved the hardness for polynomially small <em>adversarial</em> noise.  We
  also utilize the more general reductions between CLWE and classical LWE due to (Gupte
  et al., 2022). In particular, we show that if we assume the hardness of GapSVP with
  subexponential approximation factors $2^{O(d^{\delta })}$ for $\delta \in (0, 1)$,
  we can show the hardness of learning one hidden layer neural networks with polynomial
  size under Gaussian noise with $2^{-d^{\eta}}$ variance, where $\eta = \frac{\delta}{1+\delta}\in
  (0, 1/2)$. The current state-of-the-art algorithm for GapSVP is the celebrated Lenstra-Lenstra-Lovász
  (LLL) lattice basis reduction algorithm (Lenstra et al., 1982) which has approximation
  factor $2^{\Theta(d)}$. Hence, our results show  that any polynomial time learning
  algorithm for one hidden layer neural networks for any variance of noise $\sigma^2
  \ge 2^{-o(\sqrt{d})}$ would imply a major algorithmic breakthrough in the theory
  of lattices. '
openreview: gFViGKoKDq
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li25a
month: 0
tex_title: On the Hardness of Learning One Hidden Layer Neural Networks
firstpage: 700
lastpage: 701
page: 700-701
order: 700
cycles: false
bibtex_author: Li, Shuchen and Zadik, Ilias and Zampetakis, Manolis
author:
- given: Shuchen
  family: Li
- given: Ilias
  family: Zadik
- given: Manolis
  family: Zampetakis
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/li25a/li25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
