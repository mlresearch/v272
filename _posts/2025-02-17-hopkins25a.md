---
title: Do PAC-Learners Learn the Marginal Distribution?
abstract: 'The Fundamental Theorem of PAC Learning asserts that learnability of a
  concept class $H$ is equivalent to the *uniform convergence* of empirical error
  in $H$ to its mean, or equivalently, to the problem of *density estimation*, learnability
  of the underlying marginal distribution with respect to events in $H$. This seminal
  equivalence relies strongly on PAC learning’s ‘distribution-free’ assumption, that
  the adversary may choose any marginal distribution over data. Unfortunately, the
  distribution-free model is known to be overly adversarial in practice, failing to
  predict the success of modern machine learning algorithms, but without the Fundamental
  Theorem our theoretical understanding of learning under distributional constraints
  remains highly limited. In this work, we revisit the connection between PAC learning,
  uniform convergence, and density estimation beyond the distribution-free setting
  when the adversary is restricted to choosing a marginal distribution from a known
  family $\mathscr{P}$. We prove that while the traditional Fundamental Theorem fails,
  a finer-grained connection between the three fundamental notions continues to hold:
  1. PAC-Learning is strictly sandwiched between two relaxed models of density estimation,
  differing only in whether the learner knows the set of well-estimated events in
  $H$. 2. Under reasonable assumptions on $H$ and $\mathscr{P}$, density estimation
  is equivalent to *uniform estimation*, a weakening of uniform convergence allowing
  non-empirical estimators. Together, our results give a clearer picture of how the
  Fundamental Theorem extends beyond the distribution-free setting and shed new light
  on the classically challenging problem of learning under arbitrary distributional
  assumptions.'
openreview: JKVYCLDdgp
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hopkins25a
month: 0
tex_title: Do PAC-Learners Learn the Marginal Distribution?
firstpage: 581
lastpage: 610
page: 581-610
order: 581
cycles: false
bibtex_author: Hopkins, Max and Kane, Daniel and Lovett, Shachar and Mahajan, Gaurav
author:
- given: Max
  family: Hopkins
- given: Daniel
  family: Kane
- given: Shachar
  family: Lovett
- given: Gaurav
  family: Mahajan
date: 2025-02-17
address:
container-title: Proceedings of The 36th International Conference on Algorithmic Learning
  Theory
volume: '272'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v272/main/assets/hopkins25a/hopkins25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
