@Proceedings{ALT-2025,
    booktitle = {Proceedings of The 36th International Conference on Algorithmic Learning Theory},
    name = {Algorithmic Learning Theory},
    shortname = {ALT},
    editor = {Kamath, Gautam and Loh, Po-Ling},
    volume = {272},
    year = {2025},
    start = {2025-02-24},
    end = {2025-02-27},
    published = {2025-02-17},
    conference_url = {http://algorithmiclearningtheory.org/alt2025/},
    address = {Politecnico di Milano, Milan, Italy},
    conference_number = {36}
}

@inproceedings{kamath25,
  author        = {Kamath, Gautam and Loh, Po-Ling},
  title         = {Algorithmic Learning Theory 2025: Preface},
  pages         = {1 - 3}
}

@inproceedings{abeille25,
  author        = {Abeille, Marc and Janz, David and Pike-Burke, Ciara},
  title         = {When and why randomised exploration works (in linear bandits)},
  pages         = {4 - 22},
  abstract      = {We provide an approach for the analysis of randomised exploration algorithms like Thompson sampling that does not rely on forced optimism or posterior inflation. With this, we demonstrate that in the $d$-dimensional linear bandit setting, when the action space is smooth and strongly convex, randomised exploration algorithms enjoy an $n$-step regret bound of the order $O(d\sqrt{n} \log(n))$. 
  Notably, this shows for the first time that there exist non-trivial linear bandit settings where Thompson sampling can achieve optimal dimension dependence in the regret.},
  openreview    = {pT05NE9KNs}
}

@inproceedings{abeles25,
  author        = {Ab\'el\`es, Baptiste and Clerico, Eugenio and Neu, Gergely},
  title         = {Generalization bounds for mixing processes via delayed {online-to-PAC} conversions},
  pages         = {We study the generalization error of statistical learning algorithms in a non i.i.d.~setting, where the training data is sampled from a stationary mixing process. We develop an analytic framework for this scenario based on a reduction to online learning with delayed feedback. In particular, we show that the existence of an online learning algorithm with bounded regret (against a fixed statistical learning algorithm in a specially constructed game of online learning with delayed feedback) implies low generalization error of said statistical learning method even if the data sequence is sampled from a mixing time series. The rates demonstrate a trade-off between the amount of delay in the online learning game and the degree of dependence between consecutive data points, with near-optimal rates recovered in a number of well-studied settings when the delay is tuned appropriately as a function of the mixing time of the process.},
  abstract      = {23 - 40},
  openreview    = {FsI3wb6lG0}
}

@inproceedings{afzali25,
  author        = {Afzali, Mohammad and Ashtiani, Hassan and Liaw, Christopher},
  title         = {Agnostic Private Density Estimation for GMMs via List Global Stability},
  pages         = {41 - 66},
  abstract      = {We consider the problem of private density estimation for mixtures of unbounded high-dimensional Gaussians in the agnostic setting. We prove the first upper bound on the sample complexity of this problem. Previously, private learnability of high dimensional GMMs was only known in the realizable setting Afzali et al. (2024). To prove our result, we exploit the notion of \textit{list global stability} Ghazi et al. (2021) that was originally introduced in the context of supervised learning. We define an agnostic variant of this definition, showing that its existence is sufficient for agnostic private density estimation. We then construct an agnostic list globally stable learner for GMMs.},
  openreview    = {wBux6OKfjz}
}

@inproceedings{ashkezari25,
  author        = {Ashkezari, Sajad and Urner, Ruth},
  title         = {Refining the Sample Complexity of  Comparative Learning},
  pages         = {67 - 88},
  abstract      = {Comparative learning, a recently introduced variation of the PAC (Probably Approximately Correct) framework, interpolates between the two standard extreme settings of realizable and agnostic PAC learning. In comparative learning the labeling is assumed to be from one hypothesis class (the source class) while the learner's performance is to be measured against another hypothesis class (the benchmark class). This setup allows for incorporating more specific prior knowledge into PAC type learning bounds, which are known to be otherwise overly pessimistic. It also naturally represents model distillation tasks, where a predictor with specific requirements (such as being bounded in size or being interpretable) is trained on the labels from another model. A first sample complexity analysis of comparative learning established upper and lower bounds for general comparative learning. In this work, we propose a more fine grained view on this setting, distinguishing between proper learning and general (improper) learning. We derive novel upper and lower sample complexity bounds for both settings. In particular, we identify conditions for each of the regimes, and thereby exhibit how the rate depends on the relatedness of the two classes in perhaps unexpected ways.},
  openreview    = {KPVneVjxMO}
}

@inproceedings{asilis25a,
  author        = {Asilis, Julian and H{\o}gsgaard, Mikael M{\o}ller and Velegkas, Grigoris},
  title         = {Understanding Aggregations of Proper Learners in Multiclass Classification},
  pages         = {89 - 111},
  abstract      = {Multiclass learnability is known to exhibit a properness barrier: there are learnable classes which cannot be learned by any proper learner. Binary classification faces no such barrier for learnability, but a similar one for optimal learning, which can in general only be achieved by improper learners. Fortunately, recent advances in binary classification have demonstrated that this requirement can be satisfied using aggregations of proper learners, some of which are strikingly simple. This raises a natural question: to what extent can simple aggregations of proper learners overcome the properness barrier in multiclass classification? 

We give a positive answer to this question for classes which have finite Graph dimension, $d_G$. Namely, we demonstrate that the optimal binary learners of Hanneke, Larsen, and Aden-Ali et al.\ (appropriately generalized to the multiclass setting) achieve sample complexity $O\left( \frac{d_G + \ln(1 / \delta)}{\varepsilon}\right)$. This forms a strict improvement upon the sample complexity of ERM. We complement this with a lower bound demonstrating that for certain classes of Graph dimension $d_G$, majorities of ERM learners require $\Omega \left( \frac{d_G + \ln(1 / \delta)}{\varepsilon}\right)$ samples. Furthermore, we show that a single ERM requires $\Omega \left(\frac{d_G \ln(1 / \varepsilon) + \ln(1 / \delta)}{\varepsilon}\right)$ samples on such classes, exceeding the lower bound of Daniel et al.\ (2015) by a factor of $\ln(1 / \varepsilon)$. For multiclass learning in full generality  --- i.e., for classes of finite DS dimension but possibly infinite Graph dimension ---  we give a strong refutation to these learning strategies, by exhibiting a learnable class which cannot be learned to constant error by any aggregation of a finite number of proper learners.},
  openreview    = {8hTHFXxMMd}
}

@inproceedings{asilis25b,
  author        = {Asilis, Julian and Devic, Siddartha and Dughmi, Shaddin and Sharan, Vatsal and Teng, Shang-Hua},
  title         = {Proper Learnability and the Role of Unlabeled Data},
  pages         = {112 - 133},
  abstract      = {Proper learning refers to the setting in which learners must emit predictors in the underlying hypothesis class $\mathcal{H}$, and often leads to learners with simple algorithmic forms (e.g., empirical risk minimization (ERM), structural risk minimization (SRM)). The limitation of proper learning, however, is that there exist problems which can only be learned improperly, e.g.\ in multiclass classification. Thus, we ask: Under what assumptions on the hypothesis class or the information provided to the learner is a problem properly learnable? We first demonstrate that when the unlabeled data distribution is given, there always exists an optimal proper learner governed by \emph{distributional regularization}, a randomized generalization of regularization. We refer to this setting as the \emph{distribution-fixed} PAC model, and continue to evaluate the learner on its worst-case performance over all distributions. Our result holds for all metric loss functions and any finite learning problem (with no dependence on its size). Further, we demonstrate that sample complexities in the distribution-fixed PAC model can shrink by only a logarithmic factor from the classic PAC model, strongly refuting the role of unlabeled data in PAC learning (from a worst-case perspective).

We complement this with impossibility results which obstruct any characterization of proper learnability in the classic (realizable) PAC model. First, we observe that there are problems whose proper learnability is logically \emph{undecidable}, i.e., independent of the ZFC axioms. We then show that proper learnability is not a monotone property of the underlying hypothesis class, and that it is not a \emph{local} property (in a precise sense). We also point out how the non-monotonicity of proper learning obstructs relaxations of the distribution-fixed model that preserve proper learnability, including natural notions of class-conditional learning of the unlabeled data distribution.  Our impossibility results all hold even for the fundamental setting of multiclass classification, and go through a reduction of EMX learning (Ben-David et al., 2019) to proper classification which may be of independent interest.},
  openreview    = {lEhg21XO0R}
}

@inproceedings{attias25,
  author        = {Attias, Idan and Hanneke, Steve and Ramaswami, Arvind},
  title         = {Sample Compression Scheme Reductions},
  pages         = {134 - 162},
  abstract      = {We present novel reductions from sample compression schemes in multiclass classification, regression, and adversarially robust learning settings to binary sample compression schemes. Assuming we have a compression scheme for binary classes of size $f(d_\mathrm{VC})$, where $d_\mathrm{VC}$ is the VC dimension, then we have the following results: (1) If the binary compression scheme is a majority vote or a stable compression scheme, then there exists a multiclass compression scheme of size $O(f(d_\mathrm{G}))$, where $d_\mathrm{G}$ is the graph dimension. Moreover, for general binary compression schemes, we obtain a compression of size $O(f(d_\mathrm{G})\log|\mathcal{Y}|)$, where $\mathcal{Y}$ is the label space. (2) If the binary compression scheme is a majority vote or a stable compression scheme, then there exists an $\epsilon$-approximate compression scheme for regression over $[0,1]$-valued functions of size $O(f(d_\mathrm{P}))$, where $d_\mathrm{P}$ is the pseudo-dimension. For general binary compression schemes, we obtain a compression of size $O(f(d_\mathrm{P})\log(1/\epsilon))$. These results would have significant implications if the sample compression conjecture, which posits that any binary concept class with a finite VC dimension admits a binary compression scheme of size $O(d_\mathrm{VC})$, is resolved (Littlestone and Warmuth, 1986; Floyd and Warmuth, 1995; Warmuth, 2003). Our results would then extend the proof of the conjecture immediately to other settings. We establish similar results for adversarially robust learning and also provide an example of a concept class that is robustly learnable but has no bounded-size compression scheme, demonstrating that learnability is not equivalent to having a compression scheme independent of the sample size, unlike in binary classification, where compression of size $2^{O(d_\mathrm{VC})}$ is attainable (Moran and Yehudayoff, 2016).},
  openreview    = {uQsn9AZPwR}
}

@inproceedings{balkanski25a,
  author        = {Balkanski, Eric and Zhu, Cherlin},
  title         = {Strategyproof Learning with Advice},
  pages         = {163 - 166},
  abstract      = {An important challenge in robust machine learning is when training data is provided by strategic sources who may intentionally report erroneous data for their own benefit. A line of work at the intersection of machine learning and mechanism design aims to deter strategic agents from reporting erroneous training data by designing learning algorithms that are strategyproof. Strategyproofness is a strong and desirable property, but it comes at a cost in the approximation ratio of even simple risk minimization problems. 
    
    In this paper, we study strategyproof regression and classification problems in a model with advice. This model is part of a recent line on mechanism design with advice where the goal is to achieve both an improved approximation ratio when the advice is correct (consistency) and a bounded approximation ratio when the advice is incorrect (robustness). We provide the first non-trivial consistency-robustness tradeoffs for strategyproof regression and classification, which hold for simple yet interesting classes of functions. For classes of constant functions, we give a deterministic and strategyproof  mechanism that is, for any $\gamma \in (0, 2]$, $1+\gamma$ consistent and $1 + 4/\gamma$ robust and provide a lower bound that shows that this tradeoff is optimal. We extend this mechanism and its guarantees to homogeneous linear regression over $\mathbb{R}$. In the binary classification problem of selecting from three or more labelings, we present strong impossibility results for both deterministic and randomized mechanism. Finally, we provide deterministic and randomized mechanisms for selecting from two labelings.},
  openreview    = {WzfgnrS87A}
}

@inproceedings{balkanski25b,
  author        = {Balkanski, Eric and Chatzitheodorou, Jason and Maggiori, Andreas},
  title         = {Cost-Free Fairness in Online Correlation Clustering},
  pages         = {167 - 203},
  abstract      = {In the correlation clustering problem, the input is a signed graph where the sign indicates whether pairs of nodes should be placed in the same cluster or not. The goal is to create a clustering that minimizes the number of disagreements with these signs.
Correlation clustering is a key unsupervised learning problem with many practical applications. It has been widely studied in various settings, including versions with fairness constraints and cases where nodes arrive online.
In this paper, we explore a problem that combines these two settings: nodes arrive online and reveal their membership in protected groups upon arrival. We are only allowed to output fair clusters, i.e., clusters where the representation of each protected group is upper bounded by a user-specified constant at the beginning of the arrival sequence.
Our aim is to maintain approximately optimal fair clustering while minimizing a node's worst-case recourse, i.e., the number of times it changes clusters.
We present an algorithm that achieves worst-case logarithmic recourse per node while maintaining a constant-factor fair approximate clustering. Additionally, our approach simplifies the algorithm and analysis used in prior work by Cohen-Addad et al. (2022) in the online setting with recourse.},
  openreview    = {XCrvIGhVqO}
}

@inproceedings{bar-on25,
  author        = {Bar-On, Yogev and Mansour, Yishay},
  title         = {Non-stochastic Bandits With Evolving Observations},
  pages         = {204 - 227},
  abstract      = {We introduce a novel online learning framework that unifies and generalizes pre-established models, such as delayed and corrupted feedback, to encompass adversarial environments where action feedback evolves over time. In this setting, the observed loss is arbitrary and may not correlate with the true loss incurred, with each round updating previous observations adversarially. We propose regret minimization algorithms for both the full-information and bandit settings, with regret bounds quantified by the average feedback accuracy relative to the true loss. Our algorithms match the known regret bounds across many special cases, while also introducing previously unknown bounds.},
  openreview    = {MvGTH3S6rN}
}

@inproceedings{blum25a,
  author        = {Blum, Avrim and Ravichandran, Kavya},
  title         = {Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits Problem},
  pages         = {228 - 245},
  abstract      = {We give nearly-tight upper and lower bounds for the <em>improving multi-armed bandits</em> problem. An instance of this problem has $k$ arms, each of whose reward function is a concave and increasing function of the <em>number of times that arm has been pulled so far</em>. We show that for any randomized online algorithm, there exists an instance on which it must suffer at least an $\Omega(\sqrt{k})$ approximation factor relative to the optimal reward. We then provide a randomized online algorithm that guarantees an $O(\sqrt{k})$ approximation factor, if it is told the maximum reward achievable by the optimal arm in advance. We then show how to remove this assumption at the cost of an extra $O(\log k)$ approximation factor, achieving an overall $O(\sqrt{k} \log k)$ approximation relative to optimal.
},
  openreview    = {oytwNAlEfY}
}

@inproceedings{blum25b,
  author        = {Blum, Avrim and Ravichandran, Kavya},
  title         = {A Model for Combinatorial Dictionary Learning and Inference},
  pages         = {246 - 288},
  abstract      = {We are often interested in decomposing complex, structured data into simple components that explain the data. The linear version of this problem is well-studied as dictionary learning and factor analysis. In this work, we propose a combinatorial model in which to study this question, motivated by the way objects occlude each other in a scene to form an image. First, we identify a property we call ``well-structuredness'' of a set of low-dimensional components which ensures that no two components in the set are <em>too</em> similar. We show how well-structuredness is sufficient for learning the set of latent components comprising a set of sample instances. We then consider the problem: given a set of components and an instance generated from some unknown subset of them, identify which parts of the instance arise from which components. We consider two variants: (1) determine the minimal number of components required to explain the instance; (2) determine the <em>correct</em> explanation for as many locations as possible. For the latter goal, we also devise a version that is robust to adversarial corruptions, with just a slightly stronger assumption on the components. Finally, we show that the learning problem is computationally infeasible in the absence of any assumptions.},
  openreview    = {nZIFBtNuJi}
}

@inproceedings{cheu25,
  author        = {Cheu, Albert and Nayak, Debanuj},
  title         = {Differentially Private Multi-Sampling from Distributions},
  pages         = {289 - 314},
  abstract      = {Many algorithms have been developed to estimate probability distributions subject to differential privacy (DP): such an algorithm takes as input independent samples from a distribution and estimates the density function in a way that is insensitive to any one sample. A recent line of work, initiated by Raskhodnikova et al. (Neurips '21),  explores a weaker objective: a differentially private algorithm that approximates a single sample from the distribution. Raskhodnikova et al. studied the sample complexity of DP \emph{single-sampling} i.e., the minimum number of samples needed to perform this task. They showed that the sample complexity of DP single-sampling is less than the sample complexity of DP learning for certain distribution classes. We define two variants of \emph{multi-sampling}, where the goal is to privately approximate $m>1$ samples. This better models the realistic scenario where synthetic data is needed for exploratory data analysis.

A baseline solution to \emph{multi-sampling} is to invoke a single-sampling algorithm $m$ times on independently drawn datasets of samples. When the data comes from a finite domain, we improve over the baseline by a factor of $m$ in the sample complexity. When the data comes from a Gaussian, Ghazi et al. (Neurips '23) show that  \emph{single-sampling} can be performed under approximate differential privacy; we show it is possible to \emph{single- and multi-sample Gaussians with known covariance subject to pure DP}. Our solution uses a variant of the Laplace mechanism that is of independent interest.

We also give sample complexity lower bounds, one for strong multi-sampling of finite distributions and another for weak multi-sampling of bounded-covariance Gaussians.},
  openreview    = {qbktUD8REC}
}

@inproceedings{choquette-choo25,
  author        = {Choquette-Choo, Christopher A. and Ganesh, Arun and Thakurta, Abhradeep Guha},
  title         = {Near-Optimal Rates for O(1)-Smooth DP-SCO with a Single Epoch and Large Batches},
  pages         = {315 - 348},
  abstract      = {In this paper we revisit the DP stochastic convex optimization (SCO) problem. For convex smooth losses, it is well-known that the canonical DP-SGD (stochastic gradient descent) achieves the optimal rate of $O\left(\frac{LR}{\sqrt{n}} + \frac{LR \sqrt{p \log(1/\delta)}}{\epsilon n}\right)$ under $(\epsilon, \delta)$-DP (Bassily et al. 2014), and also well-known that variants of DP-SGD can achieve the optimal rate in a single epoch (Feldman et al. 2019). However, the batch gradient complexity (i.e., number of adaptive optimization steps), which is important in applications like federated learning, is less well-understood. In particular, all prior work on DP-SCO requires $\Omega(n)$ batch gradient steps, multiple epochs, or convexity for privacy.

We propose an algorithm, Accelerated-DP-SRGD (stochastic recursive gradient descent), which bypasses the limitations of past work: it achieves the optimal rate for DP-SCO (up to polylog factors), in a single epoch using  $\sqrt{n}$ batch gradient steps with batch size $\sqrt{n}$, and can be made private for arbitrary (non-convex) losses via clipping. If the global minimizer is in the constraint set, we can further improve this to $n^{1/4}$ batch gradient steps with batch size $n^{3/4}$. To achieve this, our algorithm combines three key ingredients, a variant of stochastic recursive gradients (SRG), accelerated gradient descent, and correlated noise generation from DP continual counting.},
  openreview    = {nIBYq3O2HG}
}

@inproceedings{clerico25,
  author        = {Clerico, Eugenio and Farghly, Tyler and Deligiannidis, George and Guedj, Benjamin and Doucet, Arnaud},
  title         = {Generalisation under gradient descent via deterministic PAC-Bayes},
  pages         = {349 - 389},
  abstract      = {We establish disintegrated PAC-Bayesian generalisation bounds for models trained with gradient descent methods or continuous gradient flows. Contrary to standard practice in the PAC-Bayesian setting, our result applies to optimisation algorithms that are deterministic, without requiring any <em>de-randomisation</em> step. Our bounds are fully computable, depending on the density of the initial distribution and the Hessian of the training objective over the trajectory. We show that our framework can be applied to a variety of iterative optimisation algorithms, including stochastic gradient descent (SGD), momentum-based schemes, and damped Hamiltonian dynamics.},
  openreview    = {L57EeV3VKf}
}

@inproceedings{dacunha25,
  author        = {da Cunha, Arthur and Larsen, Kasper Green and Ritzert, Martin},
  title         = {Boosting, Voting Classifiers and Randomized Sample Compression Schemes},
  pages         = {390 - 404},
  abstract      = {In Boosting, we aim to leverage multiple weak learners to produce a strong learner. At the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners. While many successful Boosting algorithms, such as the iconic AdaBoost, produce voting classifiers, their theoretical performance has long remained sub-optimal: The best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners. In this work, we break this barrier by proposing a randomized Boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the sample size. We obtain this result by building a general framework that extends sample compression methods to support randomized learning algorithms based on sub-sampling.},
  openreview    = {xSEtk5AupG}
}

@inproceedings{dellerose25,
  author        = {Delle Rose, Valentino  and Kozachinskiy, Alexander and Steifer, Tomasz},
  title         = {Effective Littlestone dimension},
  pages         = {405 - 417},
  abstract      = {Delle Rose et al.~(COLT'23)  introduced an effective version of the Vapnik-Chervonenkis dimension, and showed that it characterizes improper PAC learning with total computable learners. In this paper, we introduce and study a similar effectivization of the notion of Littlestone dimension.  Finite effective Littlestone dimension is a necessary condition for computable online learning but is not a sufficient one---which we already establish for classes of the effective Littlestone dimension 2. However, the effective Littlestone dimension equals the optimal mistake bound for computable learners in two special cases: a) for classes of Littlestone dimension 1 and b) when the learner receives as additional information an upper bound on the numbers to be guessed. Interestingly, a finite effective Littlestone dimension also guarantees that the class consists only of computable functions. },
  openreview    = {86jwIpf9FZ}
}

@inproceedings{dughmi25,
  author        = {Dughmi, Shaddin and Kalayci, Yusuf Hakan and York, Grayson},
  title         = {Is Transductive Learning Equivalent to PAC Learning?},
  pages         = {418 - 443},
  abstract      = {Much of learning theory is concerned with the design and analysis of probably approximately correct (PAC) learners. The closely related transductive model of learning has recently seen more scrutiny, with its learners often used as precursors to PAC learners. Our goal in this work is to understand and quantify the exact relationship between these two models. 
First, we observe that modest extensions of existing results show the models to be essentially equivalent for realizable learning for most natural loss functions, up to low order terms in the error and sample complexity. The situation for agnostic learning appears less straightforward, with sample complexities potentially separated by a $\frac{1}{\epsilon}$ factor. This is therefore where our main contributions lie. Our results are two-fold: 
<ol>
<li> For agnostic learning with bounded losses (including, for example, multiclass classification), we show that PAC learning reduces to transductive learning at the cost of low-order terms in the error and sample complexity. This is via an adaptation of the reduction of Aden-Ali et al. (2023a) to the agnostic setting. </li>
<li> For agnostic binary classification, we show the converse: transductive learning is essentially no more difficult than PAC learning. Together with our first result this implies that the PAC and transductive models are essentially equivalent for agnostic binary classification. This is our most technical result, and involves two key steps: (a) A symmetrization argument on the agnostic one-inclusion graph (OIG) of Long (1998) to derive the worst-case agnostic transductive instance, and (b) expressing the error of the agnostic OIG algorithm for this instance in terms of the empirical Rademacher complexity of the class. </li>
</ol>
We leave as an intriguing open question whether our second result can be extended beyond binary classification to show the transductive and PAC models equivalent more broadly.},
  openreview    = {2XAAdvSlP2}
}

@inproceedings{fishelson25,
  author        = {Fishelson, Maxwell and Kleinberg, Robert and Okoroafor, Princewill and Paes Leme, Renato and Schneider, Jon and Teng, Yifeng},
  title         = {Full Swap Regret and Discretized Calibration},
  pages         = {444 - 480},
  abstract      = {We study the problem of minimizing swap regret in structured normal-form games. Players have a very large (potentially infinite) number of pure actions, but each action has an embedding into $d$-dimensional space and payoffs are given by bilinear functions of these embeddings. We provide an efficient learning algorithm for this setting that incurs at most $\tilde{O}(T^{(d+1)/(d+3)})$ swap regret after $T$ rounds.

To achieve this, we introduce a new online learning problem we call <em>full swap regret minimization</em>. In this problem, a learner repeatedly takes a (randomized) action in a bounded convex $d$-dimensional action set $\mathcal{K}$ and then receives a loss from the adversary, with the goal of minimizing their regret with respect to the <em>worst-case</em> swap function mapping $\mathcal{K}$ to $\mathcal{K}$. For varied assumptions about the convexity and smoothness of the loss functions, we design algorithms with full swap regret bounds ranging from $O(T^{d/(d+2)})$ to $O(T^{(d+1)/(d+2)})$.

Finally, we apply these tools to the problem of online forecasting to minimize calibration error, showing that several notions of calibration can be viewed as specific instances of full swap regret. In particular, we design efficient algorithms for online forecasting that guarantee at most $O(T^{1/3})$ $\ell_2$-calibration error and $O(\max(\sqrt{\epsilon T}, T^{1/3}))$ <em>discretized-calibration</em> error (when the forecaster is restricted to predicting multiples of $\epsilon$).},
  openreview    = {8dclfIuMJ1}
}

@inproceedings{haddouche25,
  author        = {Haddouche, Maxime and Viallard, Paul and Simsekli, Umut and Guedj, Benjamin},
  title         = {A PAC-Bayesian Link Between Generalisation and Flat Minima},
  pages         = {481 - 511},
  abstract      = {Modern machine learning usually involves predictors in the overparameterised setting (number of trained parameters greater than dataset size), and their training yields not only good performance on training data, but also good generalisation capacity. This phenomenon challenges many theoretical results, and remains an open problem. To reach a better understanding, we provide novel generalisation bounds involving gradient terms. To do so, we combine the PAC-Bayes toolbox with Poincaré and Log-Sobolev inequalities, avoiding an explicit dependency on the dimension of the predictor space. Our results highlight the positive influence of flat minima (being minima with a neighbourhood nearly minimising the learning problem as well) on generalisation performance, involving directly the benefits of the optimisation phase.},
  openreview    = {P4OJds0sdD}
}

@inproceedings{hanneke25a,
  author        = {Hanneke, Steve and Yang, Liu and Wang, Gongju and Song, Yulun},
  title         = {Reliable Active Apprenticeship Learning},
  pages         = {512 - 538},
  abstract      = {We propose a learning problem, which we call reliable active apprenticeship learning, for which we define a learning algorithm providing optimal performance guarantees, which we further show are sharply characterized by the eluder dimension of a policy class. In this setting, a learning algorithm is tasked with behaving optimally in an unknown environment given by a Markov decision process. The correct actions are specified by an unknown optimal policy in a given policy class. The learner initially does not know the optimal policy, but it has the ability to query an expert, which returns the optimal action for the current state. A learner is said to be reliable if, whenever it takes an action without querying the expert, its action is guaranteed to be optimal. We are then interested in designing a reliable learner which does not query the expert too often. We propose a reliable learning algorithm which provably makes the minimal possible number of queries, which we show is precisely characterized by the eluder dimension of the policy class. We further extend this to allow for imperfect experts, modeled as an oracle with noisy responses. We study two variants of this, inspired by noise conditions from classification: namely, Massart noise and Tsybakov noise. In both cases, we propose a reliable learning strategy which achieves a nearly-minimal number of queries, and prove upper and lower bounds on the optimal number of queries in terms of the noise conditions and the eluder dimension of the policy class.},
  openreview    = {JlgUg6SKAJ}
}

@inproceedings{hanneke25b,
  author        = {Hanneke, Steve and Shaeiri, Amirreza and Wang, Hongao},
  title         = {For Universal Multiclass Online Learning, Bandit Feedback and Full Supervision are Equivalent},
  pages         = {539 - 559},
  abstract      = {We study the problem of multiclass online learning under \emph{bandit feedback} within the framework of \emph{universal learning} [Bousquet, Hanneke, Moran, van Handel, and Yehudayoff; STOC '21].
    \par
    In multiclass online learning under bandit feedback, it is well known that no concept class $\mathcal{C}$ is \emph{uniformly} learnable when the effective label space is unbounded, or in other words, no online learner guarantees a finite bound on the expected number of mistakes holding uniformly over all realizable data sequences. In contrast, surprisingly, we show that in the case of \emph{universal} learnability of concept classes $\mathcal{C}$, there is an exact equivalence between multiclass online learnability under bandit feedback and full supervision, in both the realizable and agnostic settings.
    \par
    More specifically, our first main contribution is a theory that establishes an inherent dichotomy in multiclass online learning under bandit feedback within the realizable setting. In particular, for any concept class $\mathcal{C}$ even when the effective label space is unbounded, we have: (1) If $\mathcal{C}$ does not have an infinite multiclass Littlestone tree, then there is a deterministic online learner that makes only finitely many mistakes against any realizable adversary, crucially without placing a uniform bound on the number of mistakes. (2) If $\mathcal{C}$ has an infinite multiclass Littlestone tree, then there is a strategy for the realizable adversary that forces any learner, including randomized, to make linear expected number of mistakes. Furthermore, our second main contribution reveals a similar trend in the agnostic setting.},
  openreview    = {JABal85PJe}
}

@inproceedings{hanneke25c,
  author        = {Hanneke, Steve and Wang, Kun},
  title         = {A Complete Characterization of Learnability for Stochastic Noisy Bandits},
  pages         = {560 - 577},
  abstract      = {We study the stochastic noisy bandit problem with an unknown reward function $f^*$ in a known function class $\mathcal{F}$. Formally, a model $M$ maps arms $\pi$ to a probability distribution $M(\pi)$ of reward. A model class $\mathcal{M}$ is a collection of models. For each model $M$, define its mean reward function $f^M(\pi)=\mathbb{E}_{r \sim M(\pi)}[r]$.  In the bandit learning problem, we proceed in rounds, pulling one arm $\pi$ each round and observing a reward sampled from $M(\pi)$. With knowledge of $\mathcal{M}$, supposing that the true model $M\in \mathcal{M}$, the objective is to identify an arm $\hat{\pi}$ of near-maximal mean reward $f^M(\hat{\pi})$ with high probability in a bounded number of rounds. If this is possible, then the model class is said to be learnable. 

Importantly, a result of Hanneke and Yang (2023)  shows there exist model classes for which learnability is undecidable. However, the model class they consider features deterministic rewards, and they raise the question of whether learnability is decidable for classes containing sufficiently noisy models. More formally, for any function class $\mathcal{F}$ of mean reward functions, we denote by $\mathcal{M}_{\mathcal{F}}$ the set of all models $M$ such that $f^M \in \mathcal{F}$. In other words, $\mathcal{M}_{\mathcal{F}}$ admits arbitrary zero-mean noise. Hanneke and Yang (2023)  ask the question: Can one give a simple complete characterization of which function classes $\mathcal{F}$ satisfy that $\mathcal{M}_{\mathcal{F}}$ is learnable?

For the first time, we answer this question in the positive by giving a complete characterization of learnability for model classes $\mathcal{M}_{\mathcal{F}}$. In addition to that, we also describe the full spectrum of possible optimal query complexities. Further, we prove adaptivity is sometimes necessary to achieve the optimal query complexity. Last, we revisit an important complexity measure for interactive decision making, the Decision-Estimation-Coefficient (Foster et al., 2021, 2023), and propose a new variant of the DEC which also characterizes learnability in this setting.},
  openreview    = {nv8POnFtNa}
}

@inproceedings{hogsgaard25,
  author        = {H{\o}gsgaard M{\o}ller, Mikael},
  title         = {Efficient Optimal PAC Learning},
  pages         = {578 - 580},
  abstract      = {Recent advances in the binary classification setting by Hanneke (2016) and Larsen (2023) have resulted in optimal PAC learners. These learners leverage, respectively, a clever deterministic subsampling scheme and the classic heuristic of bagging Breiman (1996). Both optimal PAC learners use, as a subroutine, the natural algorithm of empirical risk minimization. Consequently, the computational cost of these optimal PAC learners is tied to that of the empirical risk minimizer algorithm. In this work, we seek to provide an alternative perspective on the computational cost imposed by the link to the empirical risk minimizer algorithm. To this end, we show the existence of an optimal PAC learner, which offers a different tradeoff in terms of the computational cost induced by the empirical risk minimizer.},
  openreview    = {bNkKkQM9wx}
}

@inproceedings{hopkins25,
  author        = {Hopkins, Max and Kane, Daniel and Lovett, Shachar and Mahajan, Gaurav},
  title         = {Do PAC-Learners Learn the Marginal Distribution?},
  pages         = {581 - 610},
  abstract      = {The Fundamental Theorem of PAC Learning asserts that learnability of a concept class $H$ is equivalent to the *uniform convergence* of empirical error in $H$ to its mean, or equivalently, to the problem of *density estimation*, learnability of the underlying marginal distribution with respect to events in $H$. This seminal equivalence relies strongly on PAC learning's `distribution-free' assumption, that the adversary may choose any marginal distribution over data. Unfortunately, the distribution-free model is known to be overly adversarial in practice, failing to predict the success of modern machine learning algorithms, but without the Fundamental Theorem our theoretical understanding of learning under distributional constraints remains highly limited.

In this work, we revisit the connection between PAC learning, uniform convergence, and density estimation beyond the distribution-free setting when the adversary is restricted to choosing a marginal distribution from a known family $\mathscr{P}$. We prove that while the traditional Fundamental Theorem fails, a finer-grained connection between the three fundamental notions continues to hold:

1. PAC-Learning is strictly sandwiched between two relaxed models of density estimation, differing only in whether the learner knows the set of well-estimated events in $H$.

2. Under reasonable assumptions on $H$ and $\mathscr{P}$, density estimation is equivalent to *uniform estimation*, a weakening of uniform convergence allowing non-empirical estimators.

Together, our results give a clearer picture of how the Fundamental Theorem extends beyond the distribution-free setting and shed new light on the classically challenging problem of learning under arbitrary distributional assumptions.},
  openreview    = {JKVYCLDdgp}
}

@inproceedings{indyk25,
  author        = {Indyk, Piotr and Quaye, Isabelle and Rubinfeld, Ronitt and Silwal, Sandeep},
  title         = {Optimal and learned algorithms for the online list update problem with Zipfian accesses},
  pages         = {611 - 648},
  abstract      = {The online list update problem is defined as follows: we are given a list of items and the cost to access any particular item is its position from the start of the list. A sequence of item accesses come online, and our goal is to dynamically reorder the list so that the aggregate access cost is small. We study the stochastic version of the problem where the items are accessed i.i.d. from an unknown distribution $p$. The study of the stochastic version goes back at least 60 years to McCabe. 

In this paper, we first consider the simple online algorithm which swaps an accessed item with the item right before it, unless it is at the very front. This algorithm is known as the Transposition rule. We theoretically analyze the stationary behavior of Transposition and prove that its performance is within $1+o(1)$ factor of the optimal offline algorithm for access sequences sampled from heavy-tailed distributions, proving a conjecture of Rivest from 1976. 

While the stationary behavior of the Transposition rule is theoretically optimal in the aforementioned i.i.d setting, it can catastrophically fail under adversarial access sequences where only the last and second to last items are repeatedly accessed. A desirable outcome would be a policy that performs well under both circumstances. To achieve this, we use reinforcement learning to design an adaptive policy that performs well for both the i.i.d. setting and the above-mentioned adversarial access. Unsurprisingly, the learned policy appears to be an interpolation between Move-to-Front and Transposition with its behavior closer to Move-to-Front for adversarial access sequences and closer to Transposition for sequences sampled from heavy tailed distributions suggesting that the policy is adaptive and capable of responding to patterns in the access sequence.},
  openreview    = {zWpqMU5Vqc}
}

@inproceedings{kizildag25,
  author        = {K{\i}z{\i}lda\u{g}, Eren C.},
  title         = {{Information-Theoretic Guarantees for Recovering Low-Rank Tensors from Symmetric Rank-One Measurements}},
  pages         = {649 - 652},
  abstract      = {We investigate the sample complexity of recovering tensors with low symmetric rank from sym- metric rank-one measurements, a setting particularly motivated by the study of higher-order inter- actions in statistics and the analysis of two-layer polynomial neural networks. Using a covering number argument, we analyze the performance of the symmetric rank minimization program and establish near-optimal sample complexity bounds when the underlying distribution is log-concave. Our measurement model involves random symmetric rank-one tensors, leading to involved proba- bility calculations. To address these challenges, we employ the Carbery-Wright inequality, a power- ful tool for studying anti-concentration properties of random polynomials, and leverage orthogonal polynomial expansions. Additionally, we provide a sample complexity lower bound via Fano’s inequality, and discuss broader implications of our results for two-layer polynomial networks.},
  openreview    = {7Nal6i5lje}
}

@inproceedings{kontorovich25,
  author        = {Kontorovich, Aryeh and Avital, Ariel},
  title         = {Sharp bounds on aggregate expert error},
  pages         = {653 - 663},
  abstract      = {We revisit the classic problem of aggregating binary advice from conditionally independent experts, also known as the Naive Bayes setting. Our quantity of interest is the
error probability of the optimal decision rule. In the case of symmetric errors (sensitivity = specificity), reasonably tight bounds on the optimal error probability are known. In the general asymmetric case, we are not aware of any nontrivial estimates on this quantity. Our contribution consists of sharp upper and lower bounds on the optimal error probability in the general case, which recover and sharpen the best known results in the symmetric special case. Additionally, our bounds are apparently the first to take the bias into account. Since this  turns out to be closely connected to bounding the total variation distance between two product distributions, our results also have bearing on this important and challenging problem.},
  openreview    = {PQtGREBPIP}
}

@inproceedings{lau25,
  author        = {Lau, Ivan and Scarlett, Jonathan},
  title         = {Quantile Multi-Armed Bandits with 1-bit Feedback},
  pages         = {664 - 699},
  abstract      = {In this paper, we study a variant of best-arm identification involving elements of risk sensitivity and communication constraints. Specifically, the goal of the learner is to identify the arm with the highest quantile reward, while the communication from an agent (who observes rewards) and the learner (who chooses actions) is restricted to only one bit of feedback per arm pull. We propose an algorithm that utilizes noisy binary search as a subroutine, allowing the learner to estimate quantile rewards through 1-bit feedback. We derive an instance-dependent upper bound on the sample complexity of our algorithm and provide an algorithm-independent lower bound for specific instances, with the two matching to within logarithmic factors under mild conditions, or even to within constant factors in certain low error probability scaling regimes. The lower bound is applicable even in the absence of communication constraints, and thus we conclude that restricting to 1-bit feedback has a minimal impact on the scaling of the sample complexity.},
  openreview    = {u5xNVJlAse}
}

@inproceedings{li25,
  author        = {Li, Shuchen and Zadik, Ilias and Zampetakis, Manolis},
  title         = {On the Hardness of Learning One Hidden Layer Neural Networks},
  pages         = {700 - 701},
  abstract      = {In this work, we consider the problem of learning one hidden layer ReLU neural networks with inputs from $\mathbb{R}^d$. It is well known due to (Klivans and Sherstov, 2009) that without further assumptions on the distribution $\mathcal{D}$, e.g., when $\mathcal{D}$ can be supported over the Boolean hypercube, learning even one-hidden layer neural networks is impossible (or ``hard'') for polynomial-time estimators under standard cryptographic assumptions. Given the success of neural networks in practice, a long line of recent work has attempted to study instead the canonical continuous input distribution case where $\mathcal{D}$ is the isotropic Gaussian, i.e., $\mathcal{D}=\mathcal{N}\left(0,I_d\right)$ which is also the setting that we follow in this work. Yet, despite a long line of research, it remains open whether there is a polynomial-time algorithm for learning one hidden layer neural networks when $\mathcal{D} = \mathcal{N}\left(0,I_d\right)$. It is known that a single neuron, i.e., zero hidden layer neural network, can be learned in polynomial time (Zarifis et al., 2024), while neural networks with more than two hidden layers are hard to learn (Chen et al., 2022). Nevertheless, the case of one hidden layer neural networks is not well understood. 
In this paper we close this gap in the literature by answering the question of efficient learnability of neural networks with one hidden layer. We establish that under the CLWE assumption from cryptography (Bruna et al., 2021), learning the class of one hidden layer neural network with polynomial size under standard Gaussian inputs and polynomially small Gaussian noise is indeed computationally hard. Importantly, solving CLWE in polynomial time implies a polynomial-time quantum algorithm that solves the <em>worst-case</em> gap shortest vector problem (GapSVP) within polynomial factors, a widely believed hard task in cryptography and algorithmic theory of lattices (Micciancio and Regev, 2009). En route, we prove the hardness of learning Lipschitz periodic functions under standard Gaussian inputs and polynomially small Gaussian noise. This improves the previous result from (Song et al., 2021), which proved the hardness for polynomially small <em>adversarial</em> noise. 
We also utilize the more general reductions between CLWE and classical LWE due to (Gupte et al., 2022). In particular, we show that if we assume the hardness of GapSVP with subexponential approximation factors $2^{O(d^{\delta })}$ for $\delta \in (0, 1)$, we can show the hardness of learning one hidden layer neural networks with polynomial size under Gaussian noise with $2^{-d^{\eta}}$ variance, where $\eta = \frac{\delta}{1+\delta}\in (0, 1/2)$. The current state-of-the-art algorithm for GapSVP is the celebrated Lenstra-Lenstra-Lov\'{a}sz (LLL) lattice basis reduction algorithm (Lenstra et al., 1982) which has approximation factor $2^{\Theta(d)}$. Hence, our results show  that any polynomial time learning algorithm for one hidden layer neural networks for any variance of noise $\sigma^2 \ge 2^{-o(\sqrt{d})}$ would imply a major algorithmic breakthrough in the theory of lattices. },
  openreview    = {gFViGKoKDq}
}

@inproceedings{liautaud25,
  author        = {Liautaud, Paul and Gaillard, Pierre and Wintenberger, Olivier},
  title         = {Minimax-optimal and Locally-adaptive Online Nonparametric Regression},
  pages         = {702 - 735},
  abstract      = {We study adversarial online nonparametric regression with general convex losses and propose a parameter-free learning algorithm that achieves minimax optimal rates. Our approach leverages chaining trees to compete against Hölder functions and establishes optimal regret bounds. While competing with nonparametric function classes can be challenging, they often exhibit local patterns - such as local Hölder continuity - that online algorithms can exploit. Without prior knowledge, our method dynamically tracks and adapts to different Hölder profiles by pruning a core chaining tree structure, aligning itself with local smoothness variations. This leads to the first computationally efficient algorithm with locally adaptive optimal rates for online regression in an adversarial setting. Finally, we discuss how these notions could be extended to a boosting framework, offering promising directions for future research.},
  openreview    = {tKx9a84i8A}
}

@inproceedings{lok25,
  author        = {Lok, Jackie and Sonthalia, Rishi and Rebrova, Elizaveta},
  title         = {Error dynamics of mini-batch gradient descent with random reshuffling for least squares regression},
  pages         = {736 - 770},
  abstract      = {We study the discrete dynamics of mini-batch gradient descent with random reshuffling for least squares regression. We show that the training and generalization errors depend on a sample cross-covariance matrix $Z$ between the original features $X$ and a set of new features $\widetilde{X}$ in which each feature is modified by the mini-batches that appear before it during the learning process in an averaged way. Using this representation, we establish that the dynamics of mini-batch and full-batch gradient descent agree up to leading order with respect to the step size using the linear scaling rule. However, mini-batch gradient descent with random reshuffling exhibits a subtle dependence on the step size that a gradient flow analysis cannot detect, such as converging to a limit that depends on the step size. By comparing $Z$, a non-commutative polynomial of random matrices, with the sample covariance matrix of $X$ asymptotically, we demonstrate that batching affects the dynamics by resulting in a form of shrinkage on the spectrum.},
  openreview    = {XkyyvBcvA9}
}

@inproceedings{lou25,
  author        = {Lou, Mengqi and Bresler, Guy and Pananjady, Ashwin},
  title         = {Computationally efficient reductions between some statistical models},
  pages         = {771 - 771},
  abstract      = {We study the problem of approximately transforming a sample from a source statistical model to a sample from a target statistical model without knowing the parameters of the source model, and construct several computationally efficient such reductions between canonical statistical experiments. In particular, we provide computationally efficient procedures that approximately reduce uniform, Erlang, and Laplace location models to general target families. We illustrate our methodology by establishing nonasymptotic reductions between some canonical high-dimensional problems, spanning mixtures of experts, phase retrieval, and signal denoising. Notably, the reductions are structure-preserving and can accommodate missing data. We also point to a possible application in transforming one differentially private mechanism to another.},
  openreview    = {AsPOqBoQNC}
}

@inproceedings{mao25,
  author        = {Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  title         = {Enhanced $H$-Consistency Bounds},
  pages         = {772 - 813},
  abstract      = {Recent research has introduced a key notion of $H$-consistency bounds for surrogate losses. These bounds offer finite-sample guarantees, quantifying the relationship between the zero-one estimation error (or other target loss) and the surrogate loss estimation error for a specific hypothesis set. However, previous bounds were derived under the condition that a lower bound of the surrogate loss conditional regret is given as a convex function of the target conditional regret, without non-constant factors depending on the predictor or input instance. Can we derive finer and more favorable $H$-consistency bounds? In this work, we relax this condition and present a general framework for establishing <em>enhanced $H$-consistency bounds</em> based on more general inequalities relating conditional regrets. Our theorems not only subsume existing results as special cases but also enable the derivation of more favorable bounds in various scenarios. These include standard multi-class classification, binary and multi-class classification under Tsybakov noise conditions, and bipartite ranking.},
  openreview    = {qgnVGFJMJo}
}

@inproceedings{mazzetto25,
  author        = {Mazzetto, Alessio and Ceccarello, Matteo and Pietracaprina, Andrea and Pucci, Geppino and Upfal, Eli},
  title         = {Center-Based Approximation of a Drifting Distribution},
  pages         = {814 - 845},
  abstract      = {We present a novel technique for computing a center-based approximation of a drifting distribution. Given $k \geq 1$ and a stream of data, whose distribution is changing over time, the goal is to compute, at each step, the best $k$ centers representation of the current distribution, despite possibly having only a single sample from the most recent distribution. In data mining, this is traditionally attempted through the sliding-window mechanism, where the analysis is performed on the most recent fixed-size segment of the data. The problems with this approach are twofold: (1) setting the correct window size is challenging; and (2) a fixed window size cannot effectively track changes in the distribution happening at variable speed. In this paper, we propose a new methodology that dynamically adjusts the window size based on the recent drift of the data. The challenge is that it is not possible to explicitly estimate the drift, as we may have only a single data point from each distribution. Our main contribution lies in providing a rigorous mathematical analysis, establishing both an upper bound via a dynamic window size algorithm, and a lower bound that shows the tightness of our approach.},
  openreview    = {S4v8GmS5gQ}
}

@inproceedings{mitra25,
  author        = {Mitra, Siddharth and Wibisono, Andre},
  title         = {Fast Convergence of $\Phi$-Divergence Along the Unadjusted Langevin Algorithm and Proximal Sampler},
  pages         = {846 - 869},
  abstract      = {We study the mixing time of two popular discrete-time Markov chains in continuous space, the Unadjusted Langevin Algorithm and the Proximal Sampler, which are discretizations of the Langevin dynamics. We extend mixing time analyses for these Markov chains to hold in $\Phi$-divergence. We show that any $\Phi$-divergence arising from a twice-differentiable strictly convex function $\Phi$ converges to $0$ exponentially fast along these Markov chains, under the assumption that their stationary distributions satisfy the corresponding $\Phi$-Sobolev inequality, which holds for example when the target distribution of the Langevin dynamics is strongly log-concave. Our setting includes as special cases popular mixing time regimes, namely the mixing in chi-squared divergence under a Poincaré inequality, and the mixing in relative entropy under a log-Sobolev inequality. Our results follow by viewing the sampling algorithms as noisy channels and bounding the contraction coefficients arising in the appropriate strong data processing inequalities.},
  openreview    = {Q1KuxL4laL}
}

@inproceedings{pabbaraju25,
  author        = {Pabbaraju, Chirag and Sarmasarkar, Sahasrajit},
  title         = {A Characterization of List Regression},
  pages         = {870 - 920},
  abstract      = {There has been a recent interest in understanding and characterizing the sample complexity of list learning tasks, where the learning algorithm is allowed to make a short list of $k$ predictions, and we simply require one of the predictions to be correct. This includes recent works characterizing the PAC sample complexity of standard list classification and online list classification.
    
    Adding to this theme, in this work, we provide a complete characterization of list PAC {\em regression}. We propose two combinatorial dimensions, namely the $k$-OIG dimension and the $k$-fat-shattering dimension, and show that they characterize realizable and agnostic $k$-list regression respectively. These quantities generalize known dimensions for standard regression. Our work thus extends existing list learning characterizations from classification to regression.},
  openreview    = {Nh4jz04lwf}
}

@inproceedings{pinto25,
  author        = {Pinto, Andrea and Rangamani, Akshay and Poggio, Tomaso A},
  title         = {On Generalization Bounds for Neural Networks with Low Rank Layers},
  pages         = {921 - 936},
  abstract      = {While previous optimization results have suggested that deep neural networks tend to favour low-rank weight matrices, the implications of this inductive bias on generalization bounds remain underexplored. In this paper, we apply a chain rule for Gaussian complexity (Maurer, 2016a) to analyze how low-rank layers in deep networks can prevent the accumulation of rank and dimensionality factors that typically multiply across layers. This approach yields generalization bounds for rank and spectral norm constrained networks. We compare our results to prior generalization bounds for deep networks, highlighting how deep networks with low-rank layers can achieve better generalization than those with full-rank layers. Additionally, we discuss how this framework provides new perspectives on the generalization capabilities of deep networks exhibiting neural collapse.},
  openreview    = {TAvypH5yl5}
}

@inproceedings{putta25,
  author        = {Putta, Sudeep Raja and Agrawal, Shipra},
  title         = {Data Dependent Regret Bounds for Online Portfolio Selection with Predicted Returns},
  pages         = {937 - 984},
  abstract      = {We study data-dependent regret bounds for the Online Portfolio Selection (OPS) problem. As opposed to worst-case bounds that hold uniformly over all sequence of returns, data-dependent bounds adapt to the specific sequence of returns seen by the investor. Consider a market of $n$ assets and $T$ time periods, consisting of the returns $r_1,\dots,r_T \in \mathbb{R}^n_+$. The regret of our proposed algorithm, Log-Barrier Adaptive-Curvature Online Newton Step (LB-AdaCurv ONS) is bounded by $O(\min(nR\log T, \sqrt{nT\log T}))$, where  $R = \max_{t,i,j} \frac{r_t(i)}{r_t(j)}$ is a data dependent quantity that is not known to the algorithm. Thus, LB-AdaCurv ONS has a worst case regret of $O(\sqrt{nT \log T})$ while simultaneously having a data-dependent regret of $O(nR\log T)$. 

    Next, we consider the more practical setting of OPS with predicted returns, where the investor has access to predictions that can be incorporated into the portfolio selection process. We propose the Optimistic Expected Utility LB-FTRL (OUE-LB-FTRL) algorithm that incorporates the predictions using a utility function. If the predictions are accurate, OUE-LB-FTRL's regret is $O(n \log T)$ and even if the predictions are arbitrary, regret is always bounded by $O(\sqrt{nT\log T})$. We provide a meta-algorithm called Best-of-Both Worlds for OPS (BoB-OPS), that combines the portfolios of an expected utility investor and a regret minimizing investor. By properly instantiating BoB-OPS, we show that the regret with respect to the expected utility investor is $O(\log T)$ and the static regret is $O(n \log T)$.
    
    Finally, we also show new First-Order, Second-Order and Gradual-Variation regret bounds for OPS. In our analysis, we developed new regret inequalities for optimistic FTRL with convex hint functions. This extends prior work on optimistic FTRL that used only linear hints, and so could be of independent interest.},
  openreview    = {JTRjD4JeSq}
}

@inproceedings{raman25,
  author        = {Raman, Vinod and Subedi, Unique and Tewari, Ambuj},
  title         = {A Unified Theory of Supervised Online Learnability},
  pages         = {985 - 1007},
  abstract      = {We study the online learnability of hypothesis classes with respect to arbitrary, but bounded loss functions. No characterization of online learnability is known at this level of generality. In this paper, we close this gap by showing that existing techniques can be used to characterize any online learning problem with a bounded loss function. Along the way, we give a new scale-sensitive combinatorial dimension, named the Sequential Minimax dimension, that generalizes all existing dimensions in online learning theory and provides upper and lower bounds on the minimax value.},
  openreview    = {Fc3j44T06e}
}

@inproceedings{sachs25,
  author        = {Sachs, Sarah and Hadiji, Hedi and {Van Erven}, Tim and Staudigl, Mathias},
  title         = {An Online Feasible Point Method for Benign Generalized Nash Equilibrium Problems.},
  pages         = {1008 - 1040},
  abstract      = {We consider a repeatedly played generalized Nash equilibrium game. This induces a multi-agent online learning problem with joint constraints. An important challenge in this setting is that the feasible set for each agent depends on the simultaneous moves of the other agents and, therefore, varies over time. As a consequence, the agents face time-varying constraints, which are not adversarial but rather endogenous to the system. Prior work in this setting focused on convergence to a feasible solution in the limit via integrating the constraints in the objective as a penalty function.  However, no existing work can guarantee that the constraints are satisfied for all iterations while simultaneously guaranteeing convergence to a generalized Nash equilibrium. This is a problem of fundamental theoretical interest and practical relevance.
In this work, we introduce a new online feasible point method. Under the assumption that limited communication between the agents is allowed, this method guarantees feasibility. We identify the class of benign generalized Nash equilibrium problems, for which the convergence of our method to the equilibrium is guaranteed. We set this class of benign generalized Nash equilibrium games in context with existing definitions and illustrate our method with examples. 
},
  openreview    = {PFKk0M8mrS}
}

@inproceedings{schliserman25,
  author        = {Schliserman, Matan and Sherman, Uri and Koren, Tomer},
  title         = {The Dimension Strikes Back with Gradients: Generalization of Gradient Methods in Stochastic Convex Optimization},
  pages         = {1041 - 1107},
  abstract      = {We study the generalization performance of gradient methods in the fundamental stochastic convex optimization setting, focusing on its dimension dependence. 
First, for full-batch gradient descent (GD) we give a construction of a learning problem in dimension $d=O(n^2)$, where the canonical version of GD (tuned for optimal performance on the empirical risk) trained with $n$ training examples converges, with constant probability, to an approximate empirical risk minimizer with $\Omega(1)$ population excess risk.
Our bound translates to a lower bound of $\smash{\Omega(\sqrt{d})}$ on the number of training examples required for standard GD to reach a non-trivial test error, answering an open question raised by Feldman (2016) and Amir, Koren and Livni (2021) and showing that a non-trivial dimension dependence is unavoidable.
Furthermore, for standard one-pass stochastic gradient descent (SGD), we show that an application of the same construction technique provides a similar $\smash{\Omega(\sqrt{d})}$ lower bound for the sample complexity of SGD to reach a non-trivial empirical error, despite achieving optimal test performance.
This again provides for an exponential improvement in the dimension dependence compared to previous work (Koren et al., 2022), resolving an open question left therein.},
  openreview    = {Y08eqbmCDD}
}

@inproceedings{shen25,
  author        = {Shen, Jie},
  title         = {Efficient PAC Learning of Halfspaces with Constant Malicious Noise Rate},
  pages         = {1108 - 1137},
  abstract      = {Understanding noise tolerance of machine learning algorithms is a central quest in learning theory. In this work, we study the problem of computationally efficient PAC learning of halfspaces in the presence of malicious noise, where an adversary can corrupt both instances and labels of training samples. The best-known noise tolerance either depends on a target error rate under distributional assumptions or on a margin parameter under large-margin conditions. In this work, we show that when both types of conditions are satisfied, it is possible to achieve constant noise tolerance by minimizing a reweighted hinge loss. Our key ingredients include: 1) an efficient algorithm that finds weights to control the gradient deterioration from corrupted samples, and 2) a new analysis on the robustness of the hinge loss equipped with such weights.},
  openreview    = {CL5faSlqaN}
}

@inproceedings{sokolov25,
  author        = {Sokolov, Georgy and Thiessen, Maximilian and Akhmejanova, Margarita and Vitale, Fabio and Orabona, Francesco},
  title         = {Self-Directed Node Classification on Graphs},
  pages         = {1138 - 1168},
  abstract      = {We study the problem of classifying the nodes of a given graph in the self-directed learning setup. This learning setting is a variant of online learning, where rather than an adversary determining the sequence in which nodes are presented, the learner autonomously and adaptively selects them. While self-directed learning of Euclidean halfspaces, linear functions, and general multiclass hypothesis classes was recently considered, no results previously existed specifically for self-directed node classification on graphs. In this paper, we address this problem developing efficient algorithms for it. More specifically, we focus on the case of (geodesically) convex clusters, i.e., for every two nodes sharing the same label, all nodes on every shortest path between them also share the same label. In particular, we devise an algorithm with runtime polynomial in $n$ that makes only $3(h(G)+1)^4 \ln n$ mistakes on graphs with two convex clusters, where $n$ is the total number of nodes and $h(G)$ is the Hadwiger number, i.e., the size of the largest clique minor of the graph $G$. We also show that our algorithm is robust to the case that clusters are slightly non-convex, still achieving a mistake bound logarithmic in $n$. Finally, we devise a simple and efficient algorithm for homophilic clusters, where strongly connected nodes tend to belong to the same class.},
  openreview    = {QfbnsooeZB}
}

@inproceedings{srinivasan25,
  author     = {Srinivasan, Vishwak and Wibisono, Andre and Wilson, Ashia},
  title      = {High-accuracy sampling from constrained spaces with the Metropolis-adjusted Preconditioned Langevin Algorithm},
  pages      = {1169 - 1220},
  abstract   = {We propose a first-order sampling method called the Metropolis-adjusted Preconditioned Langevin Algorithm for approximate sampling from a target distribution whose support is a proper convex subset of $\mathbb{R}^{d}$.
                Our proposed method is the result of applying a Metropolis-Hastings filter to the Markov chain formed by a single step of the preconditioned Langevin algorithm with a metric $\mathscr{G}$, and is motivated by the natural gradient descent algorithm for optimisation.
                We derive non-asymptotic upper bounds for the mixing time of this method for sampling from target distributions whose potentials are bounded relative to $\mathscr{G}$, and for exponential distributions restricted to the support.
                Our analysis suggests that if $\mathscr{G}$ satisfies stronger notions of self-concordance introduced in \citet{kook2024gaussian}, then these mixing time upper bounds have a strictly better dependence on the dimension than when $\mathscr{G}$ is merely self-concordant.
                Our method is a <em>high-accuracy</em> sampler due to the polylogarithmic dependence on the error tolerance in our mixing time upper bounds.
                },
  openreview = {L68sXPqqjE}
}

@inproceedings{thuot25,
  author        = {Thuot, Victor and Carpentier, Alexandra and Giraud, Christophe and Verzelen, Nicolas},
  title         = {Clustering with bandit feedback: breaking down the computation/information gap},
  pages         = {1221 - 1284},
  abstract      = { We investigate the Clustering with Bandit feedback Problem (CBP). A learner interacts with an $N$-armed stochastic bandit with $d$-dimensional subGaussian feedback. There exists a hidden partition of the arms into $K$ groups, such that arms within the same group, share the same mean vector. The learner's task is to uncover this hidden partition with the smallest budget - i.e. the least number of observation - and with a probability of error smaller than a prescribed constant $\delta$. In this paper, (i) we derive a non asymptotic lower bound for the budget, and (ii) we introduce the computationally efficient ACB algorithm, whose budget matches the lower bound in most regimes. 
We improve on the performance of a uniform sampling strategy. Importantly, contrary to the batch setting, we establish that there is no computation-information gap in the bandit setting.},
  openreview    = {EHMcSd60XP}
}

@inproceedings{tseng25,
  author        = {Tseng, Wei-Fu and Chen, Kai-Chun and Xiao, Zi-Hong and Li, Yen-Huan},
  title         = {Online Learning of Quantum States with Logarithmic Loss via {VB}-{FTRL}},
  pages         = {1285 - 1312},
  abstract      = {Online learning of quantum states with the logarithmic loss (LL-OLQS) is a quantum generalization of online portfolio selection (OPS), a classic open problem in online learning for over three decades. This problem also emerges in designing stochastic optimization algorithms for maximum-likelihood quantum state tomography. Recently, Jezequel et al. (2022, arXiv:2209.13932) proposed the VB-FTRL algorithm, the first regret-optimal algorithm for OPS with moderate computational complexity. In this paper, we generalize VB-FTRL for LL-OLQS. Let $d$ denote the dimension and $T$ the number of rounds. The generalized algorithm achieves a regret rate of $O ( d^2 \log ( d + T ) )$ for LL-OLQS. Each iteration of the algorithm consists of solving a semidefinite program that can be implemented in polynomial time by, for example, cutting-plane methods. For comparison, the best-known regret rate for LL-OLQS is currently $O ( d^2 \log T )$, achieved by an exponential weight method. However, no explicit implementation is available for the exponential weight method for LL-OLQS. To facilitate the generalization, we introduce the notion of VB-convexity. VB-convexity is a sufficient condition for the volumetric barrier associated with any function to be convex and is of independent interest.},
  openreview    = {bwuYQ89hke}
}

@inproceedings{wang25,
  author        = {Wang, Ziao and Ghaddar, Nadim and Zhu, Banghua and Wang, Lele},
  title         = {Noisy Computing of the Threshold Function},
  pages         = {1313 - 1315},
  abstract      = {Let $\mathsf{TH}_k$ denote the $k$-out-of-$n$ threshold function: given $n$ input Boolean variables, the output is $1$ if and only if at least $k$ of the inputs are $1$. We consider the problem of computing the $\mathsf{TH}_k$ function using noisy readings of the Boolean variables, where each reading is incorrect with some fixed and known probability $p \in (0,1/2)$. As our main result, we show that it is sufficient to use $(1+o(1)) \frac{n\log \frac{m}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)}$ queries in expectation to compute the $\mathsf{TH}_k$ function with a vanishing error probability $\delta = o(1)$, where $m\triangleq \min\{k,n-k+1\}$ and $D_{\mathsf{KL}}(p \| 1-p)$ denotes the Kullback-Leibler divergence between $\mathsf{Bern}(p)$ and $\mathsf{Bern}(1-p)$ distributions. Conversely, we show that any algorithm achieving an error probability of $\delta = o(1)$ necessitates at least $(1-o(1))\frac{(n-m)\log\frac{m}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)}$ queries in expectation. The upper and lower bounds are tight when $m=o(n)$, and are within a multiplicative factor of $\frac{n}{n-m}$ when $m=\Theta(n)$. In particular, when $k=n/2$, the $\mathsf{TH}_k$ function corresponds to the $\mathsf{MAJORITY}$ function, in which case the upper and lower bounds are tight up to a multiplicative factor of two. Compared to previous work, our result tightens the dependence on $p$ in both the upper and lower bounds.},
  openreview    = {Rpy7ViuSNC}
}

@inproceedings{warmuth25,
  author        = {Warmuth, Manfred K. and Kot{\polishl}owski, Wojciech and Jones, Matt and Amid, Ehsan},
  title         = {How rotation invariant algorithms are fooled by noise on sparse targets},
  pages         = {1316 - 1360},
  abstract      = {It is well known that rotation invariant algorithms are sub-optimal for learning sparse linear problems,
when the number of examples is below the input dimension. This includes any gradient descent
trained neural net with a fully-connected input layer initialized with a rotationally symmetric
distribution. The simplest sparse problem is learning a single feature out of d features. In that case
the classification error or regression loss of rotation invariant algorithms grows with 1−n/d, where n
is the number of examples seen. These lower bounds become vacuous when the number of examples
n reaches the dimension d. After d examples, the gradient space has full rank and any weight vector
can be expressed, including the unit vector that determines the target feature.
In this work, we show that when noise is added to this sparse linear problem, rotation invariant
algorithms are still sub-optimal after seeing d or more examples. We prove this via a lower bound
for the Bayes optimal algorithm on a rotationally symmetrized problem. We then prove much
better upper bounds on the same problem for a large variety of algorithms that are non-invariant by
rotations.
Finally, we analyze the gradient flow trajectories of many standard optimization algorithms
(such as AdaGrad) on the same noisy feature learning problem, and show how they veer away from
the noisy sparse targets. We then contrast them with a group of non-rotation invariant algorithms
that veer towards the sparse targets.
We believe that the lower bounds method and trajectory categorization will be crucial for
analyzing other families of algorithms with different classes of invariances.},
  openreview    = {GSgcnyQzpz}
}

@inproceedings{zhou25,
  author        = {Zhou, Julien and Gaillard, Pierre and Rahier, Thibaud and Arbel, Julyan},
  title         = {Logarithmic Regret for Unconstrained Submodular Maximization Stochastic Bandit},
  pages         = {1361 - 1385},
  abstract      = {We address the <em>online unconstrained submodular maximization problem</em> (Online USM), in a setting with <em>stochastic bandit feedback</em>. In this framework, a decision-maker receives noisy rewards from a non monotone submodular function taking values in a known bounded interval. This paper proposes <em>Double-Greedy - Explore-then-Commit</em> (DG-ETC), adapting the Double-Greedy approach from the offline and online full-information settings. DG-ETC satisfies a $O(d\log(dT))$ problem-dependent upper bound for the $1/2$-approximate pseudo-regret, as well as a  $O(dT^{2/3}\log(dT)^{1/3})$ problem-free one at the same time, outperforming existing approaches. In particular, we introduce a problem-dependent notion of hardness characterizing the transition between logarithmic and polynomial regime for the upper bounds.},
  openreview    = {cf874WmsJC}
}

@inproceedings{zurek25,
  author        = {Zurek, Matthew and Chen, Yudong},
  title         = {The Plug-in Approach for Average-Reward and Discounted MDPs: Optimal Sample Complexity Analysis},
  pages         = {1386 - 1387},
  abstract      = {We study the sample complexity of the plug-in approach for learning $\varepsilon$-optimal policies in average-reward Markov decision processes (MDPs) with a generative model. The plug-in approach constructs a model estimate then computes an average-reward optimal policy in the estimated model.  Despite representing arguably the simplest algorithm for this problem, the plug-in approach has never been theoretically analyzed. Unlike the more well-studied discounted MDP reduction method, the plug-in approach requires no prior problem information or parameter tuning. Our results fill this gap and address the limitations of prior approaches, as we show that the plug-in approach is optimal in several well-studied settings without using prior knowledge. Specifically it achieves the optimal diameter- and mixing-based sample complexities of $\widetilde{O}\left(SA \frac{D}{\varepsilon^2}\right)$ and $\widetilde{O}\left(SA \frac{\tau_{\mathrm{unif}}}{\varepsilon^2}\right)$, respectively, without knowledge of the diameter $D$ or uniform mixing time $\tau_{\mathrm{unif}}$.
  We also obtain span-based bounds for the plug-in approach, and complement them with algorithm-specific lower bounds suggesting that they are unimprovable. Our results require novel techniques for analyzing long-horizon problems which may be broadly useful and which also improve results for the discounted plug-in approach, removing effective-horizon-related sample size restrictions and obtaining the first optimal complexity bounds for the full range of sample sizes without reward perturbation.},
  openreview    = {WXGDyFsooW}
}
